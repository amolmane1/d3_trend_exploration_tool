{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gzip\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    \"\"\"\n",
    "    this is for SKLEARN lda models only\n",
    "    plain old python object to keep the model state before pickling\n",
    "    \"\"\"\n",
    "    def __init__(self, posts=None, lda=None, vectorizer=None):\n",
    "        \"\"\"\n",
    "        inputs\n",
    "        -----\n",
    "        posts - dataset used AFTER processing (pandas)\n",
    "        lda - fitted LDA model\n",
    "        vectorizer - vectorizer used\n",
    "        note: you can get the dtm by running get_dtm(df, col, vectorizer)\n",
    "                get_dtm(posts, 'cleaned', vectorizer)\n",
    "        \"\"\"\n",
    "        self.posts = posts\n",
    "        self.lda = lda\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def save(self, fname, protocol=-1):\n",
    "        \"\"\"\n",
    "        save the object.. uses gzip for 75% reduction in size\n",
    "        \"\"\"\n",
    "        with gzip.open(fname, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f, protocol)\n",
    "            f.close()\n",
    "\n",
    "    def load(self, fname):\n",
    "        \"\"\"\n",
    "        load the gzipped object\n",
    "        \"\"\"\n",
    "\n",
    "        with gzip.open(fname, 'rb') as f:\n",
    "            tmp_dict = pickle.load(f)\n",
    "            f.close()\n",
    "        self.__dict__.update(tmp_dict)\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        use to destructure object into posts, lda, vectorizer\n",
    "        will probabably want to get the dtm, use:\n",
    "                    get_dtm(posts, 'cleaned', vectorizer)\n",
    "        usage:\n",
    "        posts, lda, vectorizer = blah.params()\n",
    "        dtm = get_dtm(posts, 'cleaned', vectorizer)\n",
    "        \"\"\"\n",
    "        return (self.posts, self.lda, self.vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You don't need to run this. This was done to set up the model and perform diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# consumer_key = <>\n",
    "# consumer_secret = <>\n",
    "# access_key = <>\n",
    "# access_secret = <>\n",
    "\n",
    "# authorization = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# authorization.set_access_token(access_key, access_secret)\n",
    "\n",
    "# client = tweepy.API(authorization)\n",
    "\n",
    "# tweets = []\n",
    "# for tweet in tweepy.Cursor(client.user_timeline, screen_name = 'bhorowitz', include_rts = False).items():\n",
    "#     if(tweet.created_at <= datetime.datetime(2017, 6, 30)):\n",
    "#         tweets.append([tweet.created_at, tweet.text])\n",
    "    \n",
    "# tweets = tweets[::-1]\n",
    "# tweets = pd.DataFrame(tweets)\n",
    "# tweets.columns = ['created_at', 'text']\n",
    "\n",
    "# K = 5\n",
    "# num_top_words_to_see = 10\n",
    "# V = 1000\n",
    "\n",
    "# vectorizer = CountVectorizer(stop_words = \"english\",\n",
    "#                              max_features = V)\n",
    "# X = vectorizer.fit_transform(tweets['text'])\n",
    "\n",
    "# lda_model = LDA(n_topics = K, \n",
    "#             learning_method = \"online\")\n",
    "# lda_model.fit(X)\n",
    "\n",
    "# document_topic_dist = lda_model.transform(X)\n",
    "# topic_word_dist = lda_model.components_\n",
    "# volcabulary_indices_words_dict = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "\n",
    "# state = State(posts = tweets, lda = lda_model, vectorizer = vectorizer)\n",
    "# state.save(fname = \"saved_lda_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_state = State()\n",
    "load_state.load(\"saved_lda_model\")\n",
    "\n",
    "tweets = load_state.params()[0]\n",
    "\n",
    "vectorizer = load_state.params()[2]\n",
    "X = vectorizer.fit_transform(tweets['text'])\n",
    "\n",
    "lda_model = load_state.params()[1]\n",
    "\n",
    "document_topic_dist = lda_model.transform(X)\n",
    "topic_word_dist = lda_model.components_\n",
    "volcabulary_indices_words_dict = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "\n",
    "K = lda_model.n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http</td>\n",
       "      <td>genius</td>\n",
       "      <td>new</td>\n",
       "      <td>post</td>\n",
       "      <td>thanks</td>\n",
       "      <td>chapter</td>\n",
       "      <td>annotated</td>\n",
       "      <td>story</td>\n",
       "      <td>wrote</td>\n",
       "      <td>9a25ur4jsp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think</td>\n",
       "      <td>know</td>\n",
       "      <td>good</td>\n",
       "      <td>doing</td>\n",
       "      <td>need</td>\n",
       "      <td>pmarca</td>\n",
       "      <td>just</td>\n",
       "      <td>quote</td>\n",
       "      <td>way</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https</td>\n",
       "      <td>great</td>\n",
       "      <td>naithanjones</td>\n",
       "      <td>yes</td>\n",
       "      <td>people</td>\n",
       "      <td>work</td>\n",
       "      <td>hard</td>\n",
       "      <td>pmarca</td>\n",
       "      <td>agree</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>jamarlinmartin</td>\n",
       "      <td>right</td>\n",
       "      <td>just</td>\n",
       "      <td>did</td>\n",
       "      <td>trying</td>\n",
       "      <td>talk</td>\n",
       "      <td>make</td>\n",
       "      <td>think</td>\n",
       "      <td>blahblahblah9tn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>don</td>\n",
       "      <td>pmarca</td>\n",
       "      <td>super</td>\n",
       "      <td>appreciate</td>\n",
       "      <td>said</td>\n",
       "      <td>a16z</td>\n",
       "      <td>sure</td>\n",
       "      <td>black</td>\n",
       "      <td>thank</td>\n",
       "      <td>nntaleb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word 1          Word 2        Word 3      Word 4  Word 5   Word 6  \\\n",
       "Topic #                                                                     \n",
       "0         http          genius           new        post  thanks  chapter   \n",
       "1        think            know          good       doing    need   pmarca   \n",
       "2        https           great  naithanjones         yes  people     work   \n",
       "3         like  jamarlinmartin         right        just     did   trying   \n",
       "4          don          pmarca         super  appreciate    said     a16z   \n",
       "\n",
       "            Word 7  Word 8 Word 9          Word 10  \n",
       "Topic #                                             \n",
       "0        annotated   story  wrote       9a25ur4jsp  \n",
       "1             just   quote    way             used  \n",
       "2             hard  pmarca  agree             true  \n",
       "3             talk    make  think  blahblahblah9tn  \n",
       "4             sure   black  thank          nntaleb  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_words_for_topics_sideways(volcabulary_indices_words_dict, topic_word_dist):\n",
    "    top_indices_of_topics = np.argsort(topic_word_dist, axis = 1)[:, np.arange(1, 10+1)*-1]\n",
    "    top_words_of_topics = pd.DataFrame(np.ndarray((K, 10)))\n",
    "    for i in range(K):\n",
    "        for j in range(10):\n",
    "            top_words_of_topics.loc[i, j] = volcabulary_indices_words_dict[top_indices_of_topics[i, j]]\n",
    "    top_words_of_topics.index.name = 'Topic #'\n",
    "    top_words_of_topics.columns = [\"Word %d\"%i for i in range(1, 10 + 1)]\n",
    "    return(top_words_of_topics)\n",
    "get_top_words_for_topics_sideways(volcabulary_indices_words_dict, topic_word_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@martina_skelly Thanks! I annotated Chapter 1 on @Genius if you want the back story: http://t.co/MyZtcRLVcf',\n",
       " \"@EmmanuelAmber in case you haven't seen it, I annotated chapter 1 on @Genius http://t.co/9a25ur4JSP\",\n",
       " '@NiamhBushnell Thanks! I annotated chapter 1 if you are interested in the backstory http://t.co/9a25ur4JSP @Genius',\n",
       " '@ebolarin Thanks! I annotated chapter 1 on @Genius if you are interested in the back story: http://t.co/9a25ur4JSP',\n",
       " '@DanielPearson Thanks! I annotated chapter 1 with photos and stuff on @genius http://t.co/t1hddcNMhX',\n",
       " '@WillGrannis Thanks! I annotated chapter 1 on @genius with photos and other stuff: http://t.co/fEutBEfm00',\n",
       " '@krmayank Thanks! I annotated chapter 1 on @genius with photos and other stuff: http://t.co/fEutBEfm00',\n",
       " '@dror_sharon Great to hear it! I annotated chapter 1 of the book at @genius http://t.co/9a25ur4JSP',\n",
       " '@genevievebos Thank you! I also told the backstory of Chapter 1 on @genius\\nif you are interested http://t.co/t1hddcNMhX',\n",
       " 'Best book party ever . . . thanks @MichaelOvitz @MCHammer @Nas @kanyewest @SteveStoute http://t.co/1eHyxjc4d9',\n",
       " 'I just decoded “The other day I threw a big barbecue at my house and invited a hundred of my closest ...” @NewsGenius http://t.co/pL7pCXQidu',\n",
       " 'I just decoded “This guy is a thug. Coming here was a big mistake” @NewsGenius http://t.co/e5HvlmiBcI',\n",
       " \"@FelixReznik @mdudas @mcuban that quote was incorrect. I was explaining why it was easy to build a co in SV not that you couldn't elsewhere.\",\n",
       " '@mguliner thanks! I wrote a new post btw: http://t.co/AVyBhgGPeA',\n",
       " 'I wrote a new post on the future of Systems Management http://t.co/NH91DnsnfX',\n",
       " '@mattkanessurbn Thanks! Btw, I annotated Chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@earthping Thanks! Btw, I annotated Chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@VaderGirl13 Thanks! Btw, I annotated Chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@CandeeJamie Thanks! If you are interested in the back story, I annotated chapter 1 on @Genius http://t.co/P7XIuvo6wB',\n",
       " '@montyvation Thanks! I annotated Chapter 1 with the back story on @genius http://t.co/t1hddcNMhX',\n",
       " '@jamesnord thanks! If you want the backstory, I annotated chapter 1 on @Genius http://t.co/tkw8lySyrR',\n",
       " 'I wrote a new post on hair http://t.co/qahPQcytlQ . Please refrain from your wise aleck comments about my hair.',\n",
       " 'I just decoded “Eighteen years later, he would be the best man at my wedding.” @NewsGenius http://t.co/tnl9enDiQp',\n",
       " 'I wrote a new post http://t.co/neq5fzDEZG with an assist from my friend @ryanleslie @TransferWise',\n",
       " '@mackfogelson now annotated at @Genius including a photo of coach Mendoza: http://t.co/9a25ur4JSP',\n",
       " 'I wrote a new post http://t.co/0rCEv6Snzm. Thank you Chuck D for the inspiration.',\n",
       " 'I wrote a new post http://t.co/0rCEv6Snzm. Thank you @MrChuckD for the inspiration.',\n",
       " 'I just decoded “Whipping was interrupted in order to pass a piece of hot wood on the buttocks of the vict...” @Genius http://t.co/SewnXT6G7R',\n",
       " 'I just decoded “Once again, certain \"cool kid\" companies and the cheerleading analysts are pretending...” @NewsGenius http://t.co/MBqw37Hyyw',\n",
       " '@bojansimic Thanks! I annotated chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@VilleKorpinen Thanks! I annotated Chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@ZisserZappy Thanks! I annotated chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@muratny thanks! I annotated chapter 1 at @genius http://t.co/9a25ur4JSP',\n",
       " '@askwheeler Thanks! I annotated chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@OguzSerdar Thanks! I annotated Chapter 1 on @Genius http://t.co/9a25ur4JSP',\n",
       " '@k_lli I have annotated chapter 1 with photos on @Genius http://t.co/9a25ur4JSP',\n",
       " 'I just decoded “And became editor of the famed New Left magazine Ramparts.” @NewsGenius http://t.co/eGODWGsXti',\n",
       " 'I just decoded “We have been happily married for nearly twenty-five years and have three wonderful ch...” @NewsGenius http://t.co/2s7X0aNxrb',\n",
       " \"@codeio in case you care, here's the actual story: http://t.co/1QSq9lsj3k\",\n",
       " 'I just decoded ““I was passing out communist literature when I was eleven years old. I remember it we...” @NewsGenius http://t.co/LjCcLDAjpI']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def view_most_representative_posts_for_topic(topic):\n",
    "    return(list(tweets.loc[np.flipud(document_topic_dist[:,topic].argsort())[0:40], 'text']))\n",
    "view_most_representative_posts_for_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = ['@Genius / @NewsGenius', 'Startup advice', 'Community / Startup advice', 'Community / Toussaint L\\'Ouverture', 'Community / Startup advice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For monthly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separated_years = [int(i.strftime('%Y')) for i in tweets['created_at']]\n",
    "separated_months = [int(i.strftime('%m')) for i in tweets['created_at']]\n",
    "tweets['ym_timestamp'] = [datetime.datetime.strptime((str(separated_years[t]) + str(separated_months[t])), \"%Y%m\")\n",
    "                                                                              for t in range(len(separated_years))]\n",
    "data_split_by_month = [group[1] for group in tweets.groupby(\"ym_timestamp\")]\n",
    "num_months = len(data_split_by_month)\n",
    "month_time_seq = [len(i) for i in data_split_by_month]\n",
    "month_indices = np.insert(values = 0, obj = 0, arr = np.cumsum(month_time_seq))\n",
    "month_timestamps = [data_split_by_month[i].iloc[0]['created_at'] for i in range(num_months)]\n",
    "month_strings = [i.strftime('%Y') + '-' + i.strftime('%m') for i in month_timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sklearn_pot_select_topics_monthly(data, document_topic_matrix, topics = [i for i in range(K)]):\n",
    "    num_selected_topics = len(topics)\n",
    "    pot_selected_topics = np.zeros((num_months, num_selected_topics))\n",
    "    # for all months,\n",
    "    for i in range(num_months):\n",
    "        # get documents for that month\n",
    "        documents_in_timestamp_range = document_topic_matrix[month_indices[i]:month_indices[i+1], topics]\n",
    "        ## set topic prop to 0 for a document if it is below 1/K\n",
    "        documents_in_timestamp_range[documents_in_timestamp_range < 1/K] = 0\n",
    "        # get doc_topic distribution, averaged over all the documents in that interval\n",
    "        pot_for_timestamp_interval = np.sum(documents_in_timestamp_range, axis = 0)/documents_in_timestamp_range.shape[0]\n",
    "        # update popularity_of_topic_over_time\n",
    "        pot_selected_topics[i,:] = pot_for_timestamp_interval\n",
    "    pot_selected_topics_df = pd.DataFrame(pot_selected_topics)\n",
    "    pot_selected_topics_df.columns = [\"%d\"%(i) for i in range(K)]\n",
    "    pot_selected_topics_df.insert(0, \"date\", month_strings)\n",
    "    return pot_selected_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_monthly = plot_sklearn_pot_select_topics_monthly(tweets, document_topic_dist)\n",
    "\n",
    "res_monthly_melted = pd.melt(res_monthly, id_vars=['date'], value_vars=[\"%d\"%(i) for i in range(K)])\n",
    "res_monthly_melted.to_csv(\"pot_lda_monthly_melted.csv\")\n",
    "\n",
    "month_time_seq_df = pd.DataFrame(month_time_seq)\n",
    "month_time_seq_df.columns = [\"num_posts_in_month\"]\n",
    "month_time_seq_df.to_csv(\"num_posts_in_month.csv\")\n",
    "\n",
    "month_indices_df = pd.DataFrame(month_indices)\n",
    "month_indices_df.columns = [\"month_indices\"]\n",
    "month_indices_df.to_csv(\"month_indices.csv\")\n",
    "\n",
    "month_strings_df = pd.DataFrame(month_strings)\n",
    "month_strings_df.columns = [\"date\"]\n",
    "month_strings_df.to_csv(\"month_strings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For quarterly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# separated_years = [int(i.strftime('%Y')) for i in tweets['created_at']]\n",
    "# separated_end_of_quarter_months = [(((int(i.strftime('%m'))-1)//3)+1)*3 for i in tweets['created_at']]\n",
    "# tweets['yq_timestamp'] = [datetime.datetime.strptime((str(separated_years[t]) + str(separated_end_of_quarter_months[t])), \"%Y%m\")\n",
    "#                                                                               for t in range(len(separated_years))]\n",
    "# data_split_by_quarter = [group[1] for group in tweets.groupby(\"yq_timestamp\")]\n",
    "# num_quarters = len(data_split_by_quarter)\n",
    "# quarter_time_seq = [len(i) for i in data_split_by_quarter]\n",
    "# quarter_indices = np.insert(values = 0, obj = 0, arr = np.cumsum(quarter_time_seq))\n",
    "# quarter_timestamps = [data_split_by_quarter[i].iloc[0]['created_at'] for i in range(num_quarters)]\n",
    "# quarter_strings = [i.strftime('%Y') + '-' + str((((int(i.strftime('%m'))-1)//3)+1)*3) for i in quarter_timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_sklearn_pot_select_topics_quarterly(data, document_topic_matrix, topics = [i for i in range(K)]):\n",
    "#     num_selected_topics = len(topics)\n",
    "#     pot_selected_topics = np.zeros((num_quarters, num_selected_topics))\n",
    "#     # for all quarters,\n",
    "#     for i in range(num_quarters):\n",
    "#         # get documents for that month\n",
    "#         documents_in_timestamp_range = document_topic_matrix[quarter_indices[i]:quarter_indices[i+1], topics]\n",
    "#         ## set topic prop to 0 for a document if it is below 1/K\n",
    "#         documents_in_timestamp_range[documents_in_timestamp_range < 1/K] = 0\n",
    "#         # get doc_topic distribution, averaged over all the documents in that interval\n",
    "#         pot_for_timestamp_interval = np.sum(documents_in_timestamp_range, axis = 0)/documents_in_timestamp_range.shape[0]\n",
    "#         # update popularity_of_topic_over_time\n",
    "#         pot_selected_topics[i,:] = pot_for_timestamp_interval\n",
    "#     pot_selected_topics_df = pd.DataFrame(pot_selected_topics)\n",
    "#     pot_selected_topics_df.columns = [\"%d\"%(i) for i in range(K)]\n",
    "#     pot_selected_topics_df.insert(0, \"date\", quarter_strings)\n",
    "#     return pot_selected_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# res = plot_sklearn_pot_select_topics(tweets, document_topic_dist)\n",
    "\n",
    "# res_melted = pd.melt(res, id_vars=['date'], value_vars=[\"%d\"%(i) for i in range(K)])\n",
    "# res_melted.to_csv(\"pot_lda_quarterly_melted.csv\")\n",
    "\n",
    "# quarter_time_seq_df = pd.DataFrame(quarter_time_seq)\n",
    "# quarter_time_seq_df.columns = [\"num_posts_in_quarter\"]\n",
    "# quarter_time_seq_df.to_csv(\"num_posts_in_quarter.csv\")\n",
    "\n",
    "# quarter_indices_df = pd.DataFrame(quarter_indices)\n",
    "# quarter_indices_df.columns = [\"quarter_indices\"]\n",
    "# quarter_indices_df.to_csv(\"quarter_indices.csv\")\n",
    "\n",
    "# quarter_strings_df = pd.DataFrame(quarter_strings)\n",
    "# quarter_strings_df.columns = [\"date\"]\n",
    "# quarter_strings_df.to_csv(\"quarter_strings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do this for both monthly and quarterly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names_df = pd.DataFrame(topic_names)\n",
    "topic_names_df.columns = [\"name\"]\n",
    "topic_names_df.to_csv(\"topic_names.csv\")\n",
    "\n",
    "pd.DataFrame(tweets['text']).to_csv(\"tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
